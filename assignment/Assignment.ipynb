{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/Business-Intelligence-and-Analytics/blob/master/assignment/Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UccXryI7Dp9a"
      },
      "source": [
        "# Business Intelligence and Analytics - Assignment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha1yIISnEr50"
      },
      "source": [
        "Welcome to the Assignment! You have to complete this notebook to find the answers to the 16 questions below. The data needed can be found in the data subfolder.\n",
        "\n",
        "In order to properly submit the assignment, you will have to do **both** steps mentioned below. Both of these must be done on the Moodle page, right under the forums.\n",
        "- Answer the questions on the [quiz](https://moodle.unil.ch/mod/quiz/view.php?id=1029998).\n",
        "- Submit your [notebook](https://moodle.unil.ch/mod/assign/view.php?id=1034597).\n",
        "\n",
        "Make sure to follow all instructions carefully.\n",
        "\n",
        "Good luck!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PP_moZVXBk_"
      },
      "source": [
        "### Part 1 - Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn13IH-NJkhb"
      },
      "source": [
        "**IMPORTANT:** In this part, whenever you are asked to perform an operation on the DataFrame (dropping rows, changing datatypes, etc.), you should always continue the exercise with the altered DataFrame.\n",
        "\n",
        "First, begin by importing pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPUcTLlKZoZR"
      },
      "source": [
        "#TODO: IMPORT PANDAS\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJHd9UZbW7FR"
      },
      "source": [
        "Load the dataset that contains various information about every company on the 2018 edition of the Fortune 500 list. That list is an annual ranking of the 500 largest US corporations (by revenues). The individual columns should be self-explanatory.\n",
        "\n",
        "You have to find the corresponding url yourself. The dataset is located in the assignment folder under data. Taking a look at the raw data before loading it might be useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "031zLoWPER9t"
      },
      "source": [
        "#TODO: LOAD THE DATASET AND SHOW THE FIRST FEW ROWS\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4mwStdHarvF"
      },
      "source": [
        "#### Question 1: How many rows and columns does the DataFrame contain?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaywC2XKa63o"
      },
      "source": [
        "#TODO: DISPLAY THE DIMENSIONS OF THE DATAFRAME\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFboYV5pYMZC"
      },
      "source": [
        "Now take a look at the datatypes of the columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0yckbB-YEbL"
      },
      "source": [
        "#TODO: LOAD THE DATATYPES\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEla5Rf0dPxN"
      },
      "source": [
        "We can see that there are several columns that should have a different datatype than what they currently do. Therefore you should now do the following transformations:\n",
        "\n",
        "| Columns | Wanted Datatype |\n",
        "| :---: | :---: |\n",
        "| Profits ($M) | Numerical continuous |\n",
        "| CEO Title | categorical nominal |\n",
        "| CEO Gender | categorical nominal |\n",
        "| Sector | categorical nominal |\n",
        "| Industry | categorical nominal |\n",
        "| State | categorical nominal |\n",
        "\n",
        "All invalid entries that cannot be converted should appear as null values in the DataFrame after you are done with the changes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sgLggGYZFV6"
      },
      "source": [
        "#TODO: CHANGE THE DATATYPE OF THE COLUMNS MENTIONED ABOVE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpNjdMw-eGAl"
      },
      "source": [
        "We will not need the coordinates of the companies' headquarters. Drop the appropriate columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UmgQoDDCeQCU"
      },
      "source": [
        "#TODO: DROP THE COLUMNS CONTAINING THE COORDINATES\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svYbRowdbTLz"
      },
      "source": [
        "#### Question 2: What column has the most missing values?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odx0CWM6abI6"
      },
      "source": [
        "#TODO: COMPUTE THE NUMBER OF MISSING VALUES PER COLUMN\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vz2gq2erbYKu"
      },
      "source": [
        "Drop the rows with missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V40dvDEVbgMP"
      },
      "source": [
        "#TODO: DROP THE ROWS WITH MISSING VALUES\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-JSGPn9bw2V"
      },
      "source": [
        "#### Question 3: How many rows were dropped?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FeB2v6Vbssr"
      },
      "source": [
        "#TODO: COMPUTE THE AMOUNT OF ROWS THAT WERE DROPPED\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cC-9OlDzckYS"
      },
      "source": [
        "#### Question 4: What percentage of CEOs are women? (Round to two decimals)\n",
        "\n",
        "(1.01% should be entered as 1.01)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKuFaTE0cti4"
      },
      "source": [
        "#TODO: COMPUTE THE PERCENTAGE OF WOMEN CEOs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2M4GRWehmkz"
      },
      "source": [
        "We are now interested in the change in rank each company made over the past year."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CZBHg6Shunq"
      },
      "source": [
        "#TODO: ADD A COLUMN THAT MEASURES HOW MANY RANKS EACH COMPANY HAS GAINED\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHr-_fC9h5UV"
      },
      "source": [
        "#### Question 5: What company gained the most ranks?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GieI8avqiCT5"
      },
      "source": [
        "#TODO: FIND THE COMPANY THAT GAINED THE MOST RANKS\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_TyQW50iOV9"
      },
      "source": [
        "### Part 2 - Regression \n",
        "\n",
        "For this part, you are going to build a linear regression model on crypto-currency trades. Each column of the dataset stands for: \n",
        "\n",
        "\n",
        "*   Name - the stock's ticker name\n",
        "*   Date - in format: yy-mm-dd\n",
        "*   Open - price of the stock when the market opened (NYSE data therefore all in USD)\n",
        "*   High - Highest price the stock reached that day\n",
        "*   Close - price of the stock when the market closed (NYSE data therefore all in USD)\n",
        "*   Volume - Number of shares traded that day\n",
        "*   Market -  Total amount of trades on the market that day\n",
        "*   spread - Trading metric \n",
        "\n",
        "\n",
        "\n",
        "**Task:** Build a linear regression model for the **closing price** of Bitcoin related crypto currencies using the following attributes: \n",
        "\n",
        "[\"name\", \"date\", \"open\", \"high\", \"low\", \"volume\",\t\"market\",\t\"spread\"]\n",
        "\n",
        "First, load the data and make a simple datatype conversion.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xan_rH2uZMYT"
      },
      "source": [
        "#TODO: RUN THE CELL TO FETCH DATA AND START WORKING \n",
        "url = 'https://storage.googleapis.com/unil_bia/cryptocurrencies.csv'\n",
        "df = pd.read_csv(url, parse_dates=[\"date\"],date_parser= pd.to_datetime)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYBkOgf6g1wL"
      },
      "source": [
        "#RUN TO TURN DATE DATA TO USABLE TYPE (INT)\n",
        "from datetime import datetime\n",
        "df.date=df.date.map(datetime.toordinal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rXCknO3WphB"
      },
      "source": [
        "You will now build the regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb5vdjkLnrCL"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#### Question 6: How many Bitcoin related cryptos are in the dataset?\n",
        "\n",
        " > _Hint_: Bitcoin related crypto currencies all start with \"Bitcoin\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDW6nW5NPzGp"
      },
      "source": [
        "#TODO: WRITE CODE TO GET ALL BITCOIN RELATED CRYPTO INTO A SEPARATE DATAFRAME\n",
        "#HINT: look up str.startswith\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI7tncDVF249"
      },
      "source": [
        "#MODIFICATION 29th of April 3 00:PM\n",
        "\n",
        "#TODO: WRITE CODE TO SELECT FEATURES OF INTEREST IN THE DATA SET AND APPLY APPROPRIATE TRANSFORMATION\n",
        "#HINT: use DataFrame indexing, look up the functions  .get_dummies\n",
        "#DATAFRAME WITH THE INDEPENDENT VARIABLES\n",
        "#DATAFRAME WITH THE DEPENDENT VARIABLE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BVTV_ZVn4ni"
      },
      "source": [
        "\n",
        "#### Question 7: How many rows and columns does the test set contain? (only independent variables)\n",
        ">You will have to split the data set into training and testing set with respective relative proportion of **70% and 30%** to first train your model and then assess its accuracy.\n",
        ">\n",
        "> You should shuffle your data set when splitting to render your reproduction more robust. Use **`random_state = 42`**\n",
        ">\n",
        "> Since features will not be on the same scale, you should standardize your data before modeling using a **StandardScaler**\n",
        ">\n",
        ">_Hint_: use the class sklearn.preprocessing\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v13QHjkM_UaI"
      },
      "source": [
        "#TODO: CODE TO SPLIT DATA SET IN TRAIN AND TEST, RANDOM SHUFFLING\n",
        "#HINT: look up the function model_selection.train_test_split, preprocessing.StandardScaler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLCPCZLboFB-"
      },
      "source": [
        " \n",
        "#### Question 8: What is the mean squared error of your linear model on the standardized test set? (round to 2 decimals)\n",
        "\n",
        "  >  You should also pay attention to the types of your variables some are categorical and others are not even numbers. \n",
        "  >\n",
        "  > _Hint_: there are categorical features among these attributes. You need to encode them with one-hot encoding first. Use the get_dummies function.\n",
        "\n",
        "#### Question 9: What is the mean absolute error of your linear model on the standardized test set? (round to 2 decimals)\n",
        "  > _Hint_: use the classes sklearn.train_test_split, sklearn.linear_model.LinearRegression._, and sklearn.metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2PfynqzE536"
      },
      "source": [
        "#TODO: CODE TO COMPUTE LINEAR MODEL AND ACCURACY METRICS\n",
        "#HINT: Look up linear_model,metrics.mean_squared_error,metrics.mean_absolute_error\n",
        "\n",
        "#Create linear regression object\n",
        "\n",
        "#Train the model using the training sets\n",
        "\n",
        "#Make predictions using the testing set\n",
        "\n",
        "#The coefficients\n",
        "\n",
        "#The mean squared error\n",
        "\n",
        "#The coefficient of determination: 1 is perfect prediction\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBqXXhU9YVJs"
      },
      "source": [
        "### Part 3 - kNN Classifier \n",
        "Now, for the remaining part of the assignment, you are going to be building a kNN classifier on a bank records of credit card users. The name of each column of data is self-explanatory regarding the content of that field. The purpose of this part is to identify customer segments and be able to predict the average income group of a given user. \n",
        "\n",
        "\n",
        "**Task:** Build a kNN classifier for the **Income_Category** based on all features **except**:\n",
        "\n",
        "`['CustomerID','Card_Category','Education_Level','Marital_Status']`\n",
        "\n",
        "and test various values of k between 1 and 20 to find the most suiting one. \n",
        "  \n",
        "In a first step, as always, load the dataset and display the first few rows.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mofpYtGG69E"
      },
      "source": [
        "#TODO: RUN THE CELL TO FETCH DATA AND START WORKING \n",
        "url = 'https://storage.googleapis.com/unil_bia/BankChurners.csv'\n",
        "df = pd.read_csv(url).drop('CustomerID.1', axis = 'columns')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y8HL5PN1PUQ"
      },
      "source": [
        "#TODO: CODE HERE LABEL ENCONDING \n",
        "#THIS LINE TURNS ['$120K +', 'Less than $40K', '$40K - $60K', '$60K - $80K','Unknown', '$80K - $120K'] TO [0, 4, 1, 2, 5, 3]\n",
        "#HINT: look up preprocessing.LabelEncoder()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eABulYwm64IZ"
      },
      "source": [
        "You will now build the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PgXhxAG4sHg"
      },
      "source": [
        "#TODO: CODE TO COMPUTE THE FEATURES OF INTEREST IN THE DATA SET \n",
        "#HINT: use DataFrame indexing, look up the functions datetime.toordinal, .get_dummies\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8t9MjC8oTLp"
      },
      "source": [
        "#### Question 10: How many rows and columns does the test set contain? (only independent variables)\n",
        "> You will have to split the data set into a training and testing set with respective  relative proportion of **70% and 30%** to first train your model and then assess its accuracy.\n",
        ">\n",
        "> You should shuffle your data set when splitting to render your reproduction more robust. Use **`random_state = 42`**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8KAeLw65NOX"
      },
      "source": [
        "#TODO: CODE TO SPLIT DATA SET IN TRAIN AND TEST, RANDOM SHUFFLING\n",
        "#HINT: look up the functions model_selection.train_test_split and preprocessing.MinMaxScaler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OuEusiioWta"
      },
      "source": [
        "\n",
        "#### Question 11: What k maximizes the accuracy_score on the minmax scaled test set? Test for k between 1 and 20.\n",
        "  > You should also pay attention to the types of your variables some are categorical and others are not even figures. \n",
        "  >\n",
        "  >_Hint_: there are categorical features among these attributes. You need to encode them with one-hot encoding first. Use the get_dummies function.\n",
        "  >\n",
        "  > You will have to encode you target feature to use it to build you classifier\n",
        ">\n",
        "  > As the scale in the features is different, you should standardize your data with a **MinMax** scaler to before using it. In fact when calculating the distances, you don't kNN to focus on larger scale dimensions. \n",
        ">\n",
        "  >_Hint_  : use the class sklearn.preprocessing\n",
        "\n",
        "#### Question 12: What is the accuracy for the optimal k on the minmax scaled test set? (round to 2 decimals)\n",
        "  >_Hint_: use the classes sklearn.train_test_split, sklearn.neighbors.KNeighborsClassifier, sklearn.preprocessing.LabelEncoder, and sklearn.metrics\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF_UwO0T6Kb4"
      },
      "source": [
        "#TODO: CODE TO COMPUTE LINEAR MODEL AND ACCURACY METRICS\n",
        "#HINT: Look up KNeighborsClassifier and metrics.accuracy_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kcl45yJFTglv"
      },
      "source": [
        "#TODO: CODE FOR VISUALIZATION OF K VERSUS ACCURACY\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ai9iJBwKPXU"
      },
      "source": [
        "#### Question 13: How many samples from your MinMax scaled test set were correctly classified in the ´\\$40K - $60K´ income category? \n",
        "  >_Hint_: use the class sklearn.neighbors.KNeighborsClassifier.predict and sklearn.metric.confusion_matrix\n",
        "  >\n",
        "  >_Hint_: pay close attention to the class ordering of the confusion matrix so you look at the right category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o34q5hm9_N1H"
      },
      "source": [
        "#TODO: CODE THE CONFUSION METRIC FOR YOUR OPTIMAL CLASSIFIER \n",
        "#HINT: look up the library sklearn.metrics and the function confusion_matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWAqe618ofo2"
      },
      "source": [
        "#### Question 14: Which income categories are predicted for the users with ids 77892545 and 77910120 with your optimal classifier? (k with the best accuracy)\n",
        "  >_Hint_: use the class sklearn.neighbors.KNeighborsClassifier.predict\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGr6-Y88-nso"
      },
      "source": [
        "#TODO: CODE TO SELECT IDS 77892545 AND 77910120\n",
        "#HINT: look up the function isin \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY0OND8wAjtX"
      },
      "source": [
        "#TODO: CODE TO COMPUTE THE FEATURES OF INTEREST IN THE RESULTING DATA SET \n",
        "#HINT: use DataFrame indexing .get_dummies\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5GR3MFY-hqG"
      },
      "source": [
        "#TODO: CODE TO COMPUTE THE PREDICTION USING THE K WITH THE BEST ACCURACY \n",
        "#HINT: use KNeighborsClassifier.predict, KNeighborsClassifier.predict_proba, sklearn.preprocessing.LabelEncoder.inverse_transform\n",
        "#ENCODING OF ['$120K +', 'Less than $40K', '$40K - $60K', '$60K - $80K','Unknown', '$80K - $120K'] TO [0, 4, 1, 2, 5, 3]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvYzYl_nxaOZ"
      },
      "source": [
        "#### Question 15: What are the probabilities for each of the two customers to be classified in the correct income category? (round to 2 decimals)\n",
        "  >_Hint_: use the class sklearn.neighbors KNeighborsClassifier.predict_proba\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c29Erp3Sym9P"
      },
      "source": [
        "#TODO: CODE TO COMPUTE THE PREDICTION USING THE K WITH THE BEST ACCURACY \n",
        "#HINT: use KNeighborsClassifier.predict,  KNeighborsClassifier.predict_proba\n",
        "#ENCODING OF ['$120K +', 'Less than $40K', '$40K - $60K', '$60K - $80K','Unknown', '$80K - $120K'] TO [0, 4, 1, 2, 5, 3]\n",
        "#LabelEncoder.inverse_transform(knn_model.classes_)) should give probality and class correspondance \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNsic6BsK2-z"
      },
      "source": [
        "#### Question 16: What are the probabilities for the users with IDs 77892545 and 77910120 (same as before), using a kNN Classifier with k=6, to be associated to `Less than 40K`  and `60K - 80K` respectively? (round to 2 decimals)\n",
        "  >_Hint_: use the class sklearn.neighbors KNeighborsClassifier.predict_proba\n",
        "  >\n",
        "  >_Hint_: use the class sklearn.preprocessing.LabelEncoder.Inverse_transform on KNeighborsClassifier.classes_ to get class correspondence, use the class KNeighborsClassifier.predict_proba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiXQyBv6KAP2"
      },
      "source": [
        "#TODO: CODE TO COMPUTE THE PREDICTION USING K = 6 \n",
        "#HINT: use KNeighborsClassifier.predict,  KNeighborsClassifier.predict_proba\n",
        "#ENCODING OF ['$120K +', 'Less than $40K', '$40K - $60K', '$60K - $80K','Unknown', '$80K - $120K'] TO [0, 4, 1, 2, 5, 3]\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}